"""
FastAPI Proxy for Ollama with Token Authentication
Supports multiple Ollama instances on different ports
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import httpx
from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import StreamingResponse, JSONResponse
from typing import Optional, Dict, List
import logging
import json

# Setup logging with debug support
DEBUG_MODE = os.getenv("DEBUG", "false").lower() == "true"
LOG_LEVEL = logging.DEBUG if DEBUG_MODE else logging.INFO

logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Set httpx logging level in debug mode
if DEBUG_MODE:
    logging.getLogger("httpx").setLevel(logging.DEBUG)
    logging.getLogger("uvicorn").setLevel(logging.DEBUG)
    logging.getLogger("uvicorn.access").setLevel(logging.DEBUG)
    logger.debug("ğŸ› DEBUG MODE ENABLED - Full logging active")

app = FastAPI(
    title="Ollama Proxy",
    description="Secure proxy for Ollama instances with token authentication",
    version="1.0.0"
)

# Configuration
API_TOKEN = os.getenv("API_TOKEN", "")
if not API_TOKEN:
    logger.warning("âš ï¸  API_TOKEN not set! Authentication is disabled.")
else:
    logger.debug(f"ğŸ”‘ API_TOKEN configured: {API_TOKEN[:4]}...{API_TOKEN[-4:] if len(API_TOKEN) > 8 else '***'}")

# Ollama instances configuration
# Format: "instance_name:host:port,instance_name2:host:port"
# Example: "ollama1:localhost:11434,ollama2:localhost:11435"
OLLAMA_INSTANCES = os.getenv("OLLAMA_INSTANCES", "default:localhost:11434")
logger.debug(f"ğŸ“ Raw OLLAMA_INSTANCES config: {OLLAMA_INSTANCES}")

# Parse Ollama instances
def parse_ollama_instances() -> Dict[str, str]:
    instances = {}
    for instance_config in OLLAMA_INSTANCES.split(","):
        parts = instance_config.strip().split(":")
        if len(parts) == 3:
            name, host, port = parts
            instances[name] = f"http://{host}:{port}"
            logger.debug(f"  â• Parsed instance '{name}' -> {instances[name]}")
        elif len(parts) == 2:
            # Assume localhost if only name:port given
            name, port = parts
            instances[name] = f"http://localhost:{port}"
            logger.debug(f"  â• Parsed instance '{name}' -> {instances[name]} (localhost assumed)")
    return instances

BACKENDS = parse_ollama_instances()
DEFAULT_BACKEND = list(BACKENDS.values())[0] if BACKENDS else "http://localhost:11434"

logger.info(f"ğŸ“¦ Configured backends: {BACKENDS}")
logger.info(f"ğŸ¯ Default backend: {DEFAULT_BACKEND}")


async def verify_token(authorization: Optional[str] = Header(None)):
    """Verify the API token from the Authorization header"""
    if not API_TOKEN:
        # If no token is configured, allow all requests (dev mode)
        logger.debug("ğŸ”“ No API_TOKEN configured - allowing request")
        return True
    
    if not authorization:
        logger.warning("â›” Missing Authorization header")
        raise HTTPException(
            status_code=401,
            detail="Missing Authorization header",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Support both "Bearer <token>" and just "<token>"
    token = authorization.replace("Bearer ", "").strip()
    logger.debug(f"ğŸ” Validating token: {token[:4]}...{token[-4:] if len(token) > 8 else '***'}")
    
    if token != API_TOKEN:
        logger.warning("â›” Invalid authentication token provided")
        raise HTTPException(
            status_code=403,
            detail="Invalid authentication token"
        )
    
    logger.debug("âœ… Token validated successfully")
    return True


def get_backend_url(instance_name: Optional[str] = None) -> str:
    """Get the backend URL for the specified instance or default"""
    if instance_name and instance_name in BACKENDS:
        backend = BACKENDS[instance_name]
        logger.debug(f"ğŸ¯ Selected backend '{instance_name}': {backend}")
        return backend
    logger.debug(f"ğŸ¯ Using default backend: {DEFAULT_BACKEND}")
    return DEFAULT_BACKEND


@app.get("/")
async def root():
    """Health check and information endpoint"""
    return {
        "service": "Ollama Proxy",
        "status": "running",
        "backends": list(BACKENDS.keys()),
        "authentication": "enabled" if API_TOKEN else "disabled"
    }


@app.get("/health")
async def health():
    """Health check endpoint"""
    return {"status": "healthy"}


@app.api_route("/{instance}/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"])
async def proxy_with_instance(
    instance: str,
    path: str,
    request: Request,
    authenticated: bool = Depends(verify_token)
):
    """
    Proxy requests to specific Ollama instance OR to default if instance name not recognized
    Example: /ollama1/api/tags -> routes to ollama1 backend
    Example: /api/tags -> routes to default backend as /api/tags (if 'api' is not an instance name)
    """
    # Check if the first segment is actually an instance name
    if instance in BACKENDS:
        logger.debug(f"ğŸ“¨ Received request for instance '{instance}', path: {path}")
        backend_url = BACKENDS[instance]
        target_url = f"{backend_url}/{path}"
        return await proxy_request(request, target_url, instance)
    else:
        # Treat the entire path including 'instance' as the path to default backend
        logger.debug(f"ğŸ“¨ '{instance}' is not an instance name, treating as path segment")
        full_path = f"{instance}/{path}" if path else instance
        logger.debug(f"ğŸ“¨ Routing to default backend with full path: {full_path}")
        target_url = f"{DEFAULT_BACKEND}/{full_path}"
        return await proxy_request(request, target_url, "default")


@app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"])
async def proxy_default(
    path: str,
    request: Request,
    authenticated: bool = Depends(verify_token)
):
    """
    Proxy requests to default Ollama instance
    Example: /api/tags -> routes to default backend
    """
    # Skip root and health endpoints
    if path in ["", "health"]:
        logger.debug(f"ğŸ“ Health check endpoint accessed: {path}")
        return {"status": "ok"}
    
    logger.debug(f"ğŸ“¨ Received request for default instance, path: {path}")
    target_url = f"{DEFAULT_BACKEND}/{path}"
    return await proxy_request(request, target_url, "default")


async def proxy_request(request: Request, target_url: str, instance_name: str = "unknown"):
    """
    Forward the request to the target Ollama instance
    Handles both streaming and non-streaming responses
    """
    try:
        # Get request body
        body = await request.body()
        
        # Log request details in debug mode
        if DEBUG_MODE:
            logger.debug(f"ğŸ“¤ Request details:")
            logger.debug(f"   Method: {request.method}")
            logger.debug(f"   URL: {target_url}")
            logger.debug(f"   Query params: {dict(request.query_params)}")
            logger.debug(f"   Body size: {len(body)} bytes")
            if body and len(body) < 1000:  # Only log small bodies
                try:
                    body_json = json.loads(body)
                    logger.debug(f"   Body: {json.dumps(body_json, indent=2)}")
                except:
                    logger.debug(f"   Body (raw): {body[:200]}...")
        
        # Prepare headers (exclude host and authorization)
        headers = {
            key: value
            for key, value in request.headers.items()
            if key.lower() not in ["host", "authorization"]
        }
        
        if DEBUG_MODE:
            logger.debug(f"   Headers: {json.dumps(headers, indent=2)}")
        
        logger.info(f"ğŸ”„ Proxying {request.method} to '{instance_name}': {target_url}")
        
        async with httpx.AsyncClient(timeout=300.0) as client:
            # Make the request to Ollama
            response = await client.request(
                method=request.method,
                url=target_url,
                content=body,
                headers=headers,
                params=request.query_params
            )
            
            logger.debug(f"ğŸ“¥ Response status: {response.status_code}")
            
            # Check if response is streaming (for chat/generate endpoints)
            content_type = response.headers.get("content-type", "")
            logger.debug(f"ğŸ“¥ Response content-type: {content_type}")
            
            if "text/event-stream" in content_type or "application/x-ndjson" in content_type:
                logger.debug("ğŸŒŠ Streaming response detected")
                # Stream the response
                chunk_count = 0
                async def stream_generator():
                    nonlocal chunk_count
                    async for chunk in response.aiter_bytes():
                        chunk_count += 1
                        if DEBUG_MODE and chunk_count % 10 == 0:
                            logger.debug(f"   Streamed {chunk_count} chunks so far...")
                        yield chunk
                    if DEBUG_MODE:
                        logger.debug(f"âœ… Streaming complete: {chunk_count} total chunks")
                
                return StreamingResponse(
                    stream_generator(),
                    status_code=response.status_code,
                    headers=dict(response.headers),
                    media_type=content_type
                )
            else:
                logger.debug("ğŸ“„ Regular response (non-streaming)")
                # Return regular response - pass through the raw content
                try:
                    response_data = response.json() if response.text else {}
                    if DEBUG_MODE and response.text and len(response.text) < 1000:
                        logger.debug(f"ğŸ“¥ Response body: {json.dumps(response_data, indent=2)}")
                except Exception as e:
                    logger.warning(f"âš ï¸  Could not parse response as JSON: {e}")
                    # Return raw text if JSON parsing fails
                    response_data = {"raw_response": response.text}
                
                logger.info(f"âœ… Request completed successfully: {response.status_code}")
                
                # Filter out hop-by-hop headers
                response_headers = {
                    k: v for k, v in response.headers.items()
                    if k.lower() not in ['content-encoding', 'content-length', 'transfer-encoding', 'connection']
                }
                
                return JSONResponse(
                    content=response_data,
                    status_code=response.status_code,
                    headers=response_headers
                )
    
    except httpx.ConnectError as e:
        logger.error(f"âŒ Connection error to {target_url}: {e}")
        raise HTTPException(
            status_code=502,
            detail=f"Cannot connect to Ollama backend: {str(e)}"
        )
    except httpx.TimeoutException as e:
        logger.error(f"â±ï¸  Timeout connecting to {target_url}: {e}")
        raise HTTPException(
            status_code=504,
            detail="Ollama backend timeout"
        )
    except Exception as e:
        logger.error(f"âŒ Error proxying request: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Proxy error: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"ğŸš€ Starting Ollama Proxy Server")
    logger.info(f"   Host: {host}")
    logger.info(f"   Port: {port}")
    logger.info(f"   Debug: {DEBUG_MODE}")
    logger.info(f"   Log Level: {logging.getLevelName(LOG_LEVEL)}")
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=DEBUG_MODE,
        log_level="debug" if DEBUG_MODE else "info"
    )
